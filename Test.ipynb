{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "similar-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import main_forecasting as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "mature-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "exterior-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1. model = lstm\n",
    "config1 = {\n",
    "    \"model\": 'lstm',\n",
    "    \"training\": True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "    \"best_model_path\": './ckpt/lstm.pt',  # 학습 완료 모델 저장 경로\n",
    "    \"parameter\": {\n",
    "        \"input_size\" : 1,  # 데이터 변수 개수, int\n",
    "        \"window_size\" : 48,  # input sequence의 길이, int\n",
    "        \"forecast_step\" : 24,  # 예측할 미래 시점의 길이, int\n",
    "        \"num_layers\" : 2,  # recurrent layers의 수, int(default: 2, 범위: 1 이상)\n",
    "        \"hidden_size\" : 64,  # hidden state의 차원, int(default: 64, 범위: 1 이상)\n",
    "        \"dropout\" : 0.1,  # dropout 확률, float(default: 0.1, 범위: 0 이상 1 이하)\n",
    "        \"bidirectional\" : True,  # 모델의 양방향성 여부, bool(default: True)\n",
    "        \"num_epochs\" : 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "        \"batch_size\" : 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "        \"lr\" : 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "        \"device\" : 'cuda'  # 학습 환경, (default: 'cuda', ['cuda', 'cpu'] 중 선택)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Case 2. model = gru\n",
    "config2 = {\n",
    "    \"model\": 'gru',\n",
    "    \"training\": True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "    \"best_model_path\": './ckpt/gru.pt',  # 학습 완료 모델 저장 경로\n",
    "    \"parameter\": {\n",
    "        \"input_size\" : 1,  # 데이터 변수 개수, int\n",
    "        \"window_size\" : 48,  # input sequence의 길이, int\n",
    "        \"forecast_step\" : 24,  # 예측할 미래 시점의 길이, int\n",
    "        \"num_layers\" : 2,  # recurrent layers의 수, int(default: 2, 범위: 1 이상)\n",
    "        \"hidden_size\" : 64,  # hidden state의 차원, int(default: 64, 범위: 1 이상)\n",
    "        \"dropout\" : 0.1,  # dropout 확률, float(default: 0.1, 범위: 0 이상 1 이하)\n",
    "        \"bidirectional\" : True,  # 모델의 양방향성 여부, bool(default: True)\n",
    "        \"num_epochs\" : 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "        \"batch_size\" : 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "        \"lr\" : 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "        \"device\" : 'cuda'  # 학습 환경, (default: 'cuda', ['cuda', 'cpu'] 중 선택)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Case 3. model = informer\n",
    "config3 = {\n",
    "    \"model\": 'informer',\n",
    "    \"training\": True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "    \"best_model_path\": './ckpt/informer.pt',  # 학습 완료 모델 저장 경로\n",
    "    \"parameter\": {\n",
    "        \"input_size\" : 1,  # 데이터 변수 개수, int\n",
    "        \"window_size\" : 48,  # input sequence의 길이, int\n",
    "        \"forecast_step\" : 24,  # 예측할 미래 시점의 길이, int\n",
    "        \"label_len\" : 12,  # Decoder의 start token 길이, int(default: 12)\n",
    "        \"d_model\" : 512,  # 모델의 hidden dimension, int(default: 512)\n",
    "        \"e_layers\" : 2,  # encoder layer 수, int(default: 2)\n",
    "        \"d_layers\" : 1,  # decoder layer 수, int(default: 1)\n",
    "        \"d_ff\" : 2048,  # fully connected layer의 hidden dimension, int(default: 2048)\n",
    "        \"num_epochs\" : 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "        \"batch_size\" : 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "        \"lr\" : 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "        \"device\" : 'cuda',  # 학습 환경, (default: 'cuda', ['cuda', 'cpu'] 중 선택)\n",
    "        \"factor\" : 5, # 모델의 ProbSparse Attention factor, int(default: 5)\n",
    "        \"dropout\" : 0.05, # dropout ratio, int(default: 0.05)\n",
    "        \"attn\" : 'prob', # 모델의 attention 계산 방식, (default: 'prob', ['prob', 'full'] 중 선택)\n",
    "        \"n_heads\" : 8, # multi-head attention head 수, int(default: 8)\n",
    "        \"embed\" : 'timeF', # time features encoding 방식, (default: 'timeF', ['timeF', 'fixed', 'learned'] 중 선택)\n",
    "        \"lradj\" : 'type1' # learning rate 조정 방식, (default: 'type1', ['type1', 'type2'] 중 선택)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Case 4. model = scinet\n",
    "config4 = {\n",
    "    \"model\": 'scinet',\n",
    "    \"training\": True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "    \"best_model_path\": './ckpt/scinet.pt',  # 학습 완료 모델 저장 경로\n",
    "    \"parameter\": {\n",
    "        \"input_size\" : 1,  # 데이터 변수 개수, int\n",
    "        \"window_size\" : 48,  # input sequence의 길이, int\n",
    "        \"forecast_step\" : 24,  # 예측할 미래 시점의 길이, int\n",
    "        \"levels\" : 2, # Tree의 depth, int(default: 2, 범위: input sequence의 로그 값 이하, 2~4 설정 권장)\n",
    "        \"stacks\" : 1, # SCINet 구조를 쌓는 횟수, int(default: 1, 범위: 3 이하)\n",
    "        \"num_epochs\" : 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "        \"batch_size\" : 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "        \"lr\" : 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "        \"device\" : 'cuda'  # 학습 환경, (default: 'cuda', ['cuda', 'cpu'] 중 선택)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "speaking-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset_dir = {\n",
    "    \"train\": './data/train_data.csv',\n",
    "    \"test\": './data/test_data.csv'\n",
    "}\n",
    "\n",
    "# train/test 데이터 불러오기 (csv 형태)\n",
    "# shape=(# time steps, )\n",
    "train_data = pd.read_csv(dataset_dir[\"train\"])\n",
    "train_data = train_data[\"Appliances\"].values\n",
    "\n",
    "test_data = pd.read_csv(dataset_dir[\"test\"])\n",
    "test_date = test_data[\"date\"].values\n",
    "test_data = test_data[\"Appliances\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "colonial-avatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model\n",
      "\n",
      "Epoch 1/150\n",
      "----------\n",
      "train Loss: 0.9989 RMSE: 0.9995\n",
      "val Loss: 0.9087 RMSE: 0.9533\n",
      "\n",
      "Epoch 2/150\n",
      "----------\n",
      "train Loss: 0.9976 RMSE: 0.9988\n",
      "val Loss: 0.9075 RMSE: 0.9526\n",
      "\n",
      "Epoch 3/150\n",
      "----------\n",
      "train Loss: 0.9964 RMSE: 0.9982\n",
      "val Loss: 0.9063 RMSE: 0.9520\n",
      "\n",
      "Epoch 4/150\n",
      "----------\n",
      "train Loss: 0.9952 RMSE: 0.9976\n",
      "val Loss: 0.9049 RMSE: 0.9513\n",
      "\n",
      "Epoch 5/150\n",
      "----------\n",
      "train Loss: 0.9941 RMSE: 0.9970\n",
      "val Loss: 0.9036 RMSE: 0.9506\n",
      "\n",
      "Epoch 6/150\n",
      "----------\n",
      "train Loss: 0.9928 RMSE: 0.9964\n",
      "val Loss: 0.9021 RMSE: 0.9498\n",
      "\n",
      "Epoch 7/150\n",
      "----------\n",
      "train Loss: 0.9914 RMSE: 0.9957\n",
      "val Loss: 0.9004 RMSE: 0.9489\n",
      "\n",
      "Epoch 8/150\n",
      "----------\n",
      "train Loss: 0.9900 RMSE: 0.9950\n",
      "val Loss: 0.8984 RMSE: 0.9478\n",
      "\n",
      "Epoch 9/150\n",
      "----------\n",
      "train Loss: 0.9882 RMSE: 0.9941\n",
      "val Loss: 0.8958 RMSE: 0.9465\n",
      "\n",
      "Epoch 10/150\n",
      "----------\n",
      "train Loss: 0.9861 RMSE: 0.9930\n",
      "val Loss: 0.8927 RMSE: 0.9448\n",
      "\n",
      "Epoch 11/150\n",
      "----------\n",
      "train Loss: 0.9835 RMSE: 0.9917\n",
      "val Loss: 0.8890 RMSE: 0.9429\n",
      "\n",
      "Epoch 12/150\n",
      "----------\n",
      "train Loss: 0.9805 RMSE: 0.9902\n",
      "val Loss: 0.8842 RMSE: 0.9403\n",
      "\n",
      "Epoch 13/150\n",
      "----------\n",
      "train Loss: 0.9764 RMSE: 0.9881\n",
      "val Loss: 0.8779 RMSE: 0.9370\n",
      "\n",
      "Epoch 14/150\n",
      "----------\n",
      "train Loss: 0.9717 RMSE: 0.9857\n",
      "val Loss: 0.8699 RMSE: 0.9327\n",
      "\n",
      "Epoch 15/150\n",
      "----------\n",
      "train Loss: 0.9647 RMSE: 0.9822\n",
      "val Loss: 0.8605 RMSE: 0.9276\n",
      "\n",
      "Epoch 16/150\n",
      "----------\n",
      "train Loss: 0.9566 RMSE: 0.9781\n",
      "val Loss: 0.8493 RMSE: 0.9216\n",
      "\n",
      "Epoch 17/150\n",
      "----------\n",
      "train Loss: 0.9474 RMSE: 0.9734\n",
      "val Loss: 0.8358 RMSE: 0.9142\n",
      "\n",
      "Epoch 18/150\n",
      "----------\n",
      "train Loss: 0.9362 RMSE: 0.9676\n",
      "val Loss: 0.8237 RMSE: 0.9076\n",
      "\n",
      "Epoch 19/150\n",
      "----------\n",
      "train Loss: 0.9266 RMSE: 0.9626\n",
      "val Loss: 0.8141 RMSE: 0.9023\n",
      "\n",
      "Epoch 20/150\n",
      "----------\n",
      "train Loss: 0.9187 RMSE: 0.9585\n",
      "val Loss: 0.8079 RMSE: 0.8988\n",
      "\n",
      "Epoch 21/150\n",
      "----------\n",
      "train Loss: 0.9153 RMSE: 0.9567\n",
      "val Loss: 0.8036 RMSE: 0.8964\n",
      "\n",
      "Epoch 22/150\n",
      "----------\n",
      "train Loss: 0.9123 RMSE: 0.9551\n",
      "val Loss: 0.7985 RMSE: 0.8936\n",
      "\n",
      "Epoch 23/150\n",
      "----------\n",
      "train Loss: 0.9088 RMSE: 0.9533\n",
      "val Loss: 0.7936 RMSE: 0.8909\n",
      "\n",
      "Epoch 24/150\n",
      "----------\n",
      "train Loss: 0.9059 RMSE: 0.9518\n",
      "val Loss: 0.7897 RMSE: 0.8886\n",
      "\n",
      "Epoch 25/150\n",
      "----------\n",
      "train Loss: 0.9036 RMSE: 0.9506\n",
      "val Loss: 0.7852 RMSE: 0.8861\n",
      "\n",
      "Epoch 26/150\n",
      "----------\n",
      "train Loss: 0.9012 RMSE: 0.9493\n",
      "val Loss: 0.7816 RMSE: 0.8841\n",
      "\n",
      "Epoch 27/150\n",
      "----------\n",
      "train Loss: 0.8990 RMSE: 0.9481\n",
      "val Loss: 0.7782 RMSE: 0.8822\n",
      "\n",
      "Epoch 28/150\n",
      "----------\n",
      "train Loss: 0.8971 RMSE: 0.9471\n",
      "val Loss: 0.7744 RMSE: 0.8800\n",
      "\n",
      "Epoch 29/150\n",
      "----------\n",
      "train Loss: 0.8953 RMSE: 0.9462\n",
      "val Loss: 0.7695 RMSE: 0.8772\n",
      "\n",
      "Epoch 30/150\n",
      "----------\n",
      "train Loss: 0.8934 RMSE: 0.9452\n",
      "val Loss: 0.7664 RMSE: 0.8754\n",
      "\n",
      "Epoch 31/150\n",
      "----------\n",
      "train Loss: 0.8916 RMSE: 0.9442\n",
      "val Loss: 0.7646 RMSE: 0.8744\n",
      "\n",
      "Epoch 32/150\n",
      "----------\n",
      "train Loss: 0.8900 RMSE: 0.9434\n",
      "val Loss: 0.7637 RMSE: 0.8739\n",
      "\n",
      "Epoch 33/150\n",
      "----------\n",
      "train Loss: 0.8884 RMSE: 0.9425\n",
      "val Loss: 0.7611 RMSE: 0.8724\n",
      "\n",
      "Epoch 34/150\n",
      "----------\n",
      "train Loss: 0.8870 RMSE: 0.9418\n",
      "val Loss: 0.7592 RMSE: 0.8713\n",
      "\n",
      "Epoch 35/150\n",
      "----------\n",
      "train Loss: 0.8851 RMSE: 0.9408\n",
      "val Loss: 0.7569 RMSE: 0.8700\n",
      "\n",
      "Epoch 36/150\n",
      "----------\n",
      "train Loss: 0.8834 RMSE: 0.9399\n",
      "val Loss: 0.7554 RMSE: 0.8692\n",
      "\n",
      "Epoch 37/150\n",
      "----------\n",
      "train Loss: 0.8817 RMSE: 0.9390\n",
      "val Loss: 0.7524 RMSE: 0.8674\n",
      "\n",
      "Epoch 38/150\n",
      "----------\n",
      "train Loss: 0.8797 RMSE: 0.9379\n",
      "val Loss: 0.7516 RMSE: 0.8670\n",
      "\n",
      "Epoch 39/150\n",
      "----------\n",
      "train Loss: 0.8782 RMSE: 0.9371\n",
      "val Loss: 0.7507 RMSE: 0.8664\n",
      "\n",
      "Epoch 40/150\n",
      "----------\n",
      "train Loss: 0.8764 RMSE: 0.9362\n",
      "val Loss: 0.7489 RMSE: 0.8654\n",
      "\n",
      "Epoch 41/150\n",
      "----------\n",
      "train Loss: 0.8753 RMSE: 0.9356\n",
      "val Loss: 0.7473 RMSE: 0.8644\n",
      "\n",
      "Epoch 42/150\n",
      "----------\n",
      "train Loss: 0.8739 RMSE: 0.9349\n",
      "val Loss: 0.7462 RMSE: 0.8638\n",
      "\n",
      "Epoch 43/150\n",
      "----------\n",
      "train Loss: 0.8731 RMSE: 0.9344\n",
      "val Loss: 0.7460 RMSE: 0.8637\n",
      "\n",
      "Epoch 44/150\n",
      "----------\n",
      "train Loss: 0.8716 RMSE: 0.9336\n",
      "val Loss: 0.7424 RMSE: 0.8616\n",
      "\n",
      "Epoch 45/150\n",
      "----------\n",
      "train Loss: 0.8707 RMSE: 0.9331\n",
      "val Loss: 0.7412 RMSE: 0.8609\n",
      "\n",
      "Epoch 46/150\n",
      "----------\n",
      "train Loss: 0.8699 RMSE: 0.9327\n",
      "val Loss: 0.7427 RMSE: 0.8618\n",
      "\n",
      "Epoch 47/150\n",
      "----------\n",
      "train Loss: 0.8690 RMSE: 0.9322\n",
      "val Loss: 0.7441 RMSE: 0.8626\n",
      "\n",
      "Epoch 48/150\n",
      "----------\n",
      "train Loss: 0.8689 RMSE: 0.9322\n",
      "val Loss: 0.7413 RMSE: 0.8610\n",
      "\n",
      "Epoch 49/150\n",
      "----------\n",
      "train Loss: 0.8678 RMSE: 0.9315\n",
      "val Loss: 0.7398 RMSE: 0.8601\n",
      "\n",
      "Epoch 50/150\n",
      "----------\n",
      "train Loss: 0.8675 RMSE: 0.9314\n",
      "val Loss: 0.7382 RMSE: 0.8592\n",
      "\n",
      "Epoch 51/150\n",
      "----------\n",
      "train Loss: 0.8667 RMSE: 0.9310\n",
      "val Loss: 0.7371 RMSE: 0.8585\n",
      "\n",
      "Epoch 52/150\n",
      "----------\n",
      "train Loss: 0.8659 RMSE: 0.9305\n",
      "val Loss: 0.7373 RMSE: 0.8587\n",
      "\n",
      "Epoch 53/150\n",
      "----------\n",
      "train Loss: 0.8656 RMSE: 0.9304\n",
      "val Loss: 0.7373 RMSE: 0.8587\n",
      "\n",
      "Epoch 54/150\n",
      "----------\n",
      "train Loss: 0.8662 RMSE: 0.9307\n",
      "val Loss: 0.7405 RMSE: 0.8605\n",
      "\n",
      "Epoch 55/150\n",
      "----------\n",
      "train Loss: 0.8649 RMSE: 0.9300\n",
      "val Loss: 0.7344 RMSE: 0.8570\n",
      "\n",
      "Epoch 56/150\n",
      "----------\n",
      "train Loss: 0.8642 RMSE: 0.9296\n",
      "val Loss: 0.7323 RMSE: 0.8558\n",
      "\n",
      "Epoch 57/150\n",
      "----------\n",
      "train Loss: 0.8635 RMSE: 0.9293\n",
      "val Loss: 0.7325 RMSE: 0.8559\n",
      "\n",
      "Epoch 58/150\n",
      "----------\n",
      "train Loss: 0.8626 RMSE: 0.9288\n",
      "val Loss: 0.7343 RMSE: 0.8569\n",
      "\n",
      "Epoch 59/150\n",
      "----------\n",
      "train Loss: 0.8621 RMSE: 0.9285\n",
      "val Loss: 0.7338 RMSE: 0.8566\n",
      "\n",
      "Epoch 60/150\n",
      "----------\n",
      "train Loss: 0.8616 RMSE: 0.9282\n",
      "val Loss: 0.7324 RMSE: 0.8558\n",
      "\n",
      "Epoch 61/150\n",
      "----------\n",
      "train Loss: 0.8614 RMSE: 0.9281\n",
      "val Loss: 0.7304 RMSE: 0.8546\n",
      "\n",
      "Epoch 62/150\n",
      "----------\n",
      "train Loss: 0.8611 RMSE: 0.9280\n",
      "val Loss: 0.7325 RMSE: 0.8559\n",
      "\n",
      "Epoch 63/150\n",
      "----------\n",
      "train Loss: 0.8595 RMSE: 0.9271\n",
      "val Loss: 0.7310 RMSE: 0.8550\n",
      "\n",
      "Epoch 64/150\n",
      "----------\n",
      "train Loss: 0.8589 RMSE: 0.9268\n",
      "val Loss: 0.7295 RMSE: 0.8541\n",
      "\n",
      "Epoch 65/150\n",
      "----------\n",
      "train Loss: 0.8577 RMSE: 0.9261\n",
      "val Loss: 0.7294 RMSE: 0.8540\n",
      "\n",
      "Epoch 66/150\n",
      "----------\n",
      "train Loss: 0.8571 RMSE: 0.9258\n",
      "val Loss: 0.7276 RMSE: 0.8530\n",
      "\n",
      "Epoch 67/150\n",
      "----------\n",
      "train Loss: 0.8562 RMSE: 0.9253\n",
      "val Loss: 0.7253 RMSE: 0.8516\n",
      "\n",
      "Epoch 68/150\n",
      "----------\n",
      "train Loss: 0.8561 RMSE: 0.9252\n",
      "val Loss: 0.7251 RMSE: 0.8515\n",
      "\n",
      "Epoch 69/150\n",
      "----------\n",
      "train Loss: 0.8550 RMSE: 0.9246\n",
      "val Loss: 0.7263 RMSE: 0.8522\n",
      "\n",
      "Epoch 70/150\n",
      "----------\n",
      "train Loss: 0.8541 RMSE: 0.9242\n",
      "val Loss: 0.7269 RMSE: 0.8526\n",
      "\n",
      "Epoch 71/150\n",
      "----------\n",
      "train Loss: 0.8531 RMSE: 0.9236\n",
      "val Loss: 0.7295 RMSE: 0.8541\n",
      "\n",
      "Epoch 72/150\n",
      "----------\n",
      "train Loss: 0.8526 RMSE: 0.9234\n",
      "val Loss: 0.7284 RMSE: 0.8534\n",
      "\n",
      "Epoch 73/150\n",
      "----------\n",
      "train Loss: 0.8521 RMSE: 0.9231\n",
      "val Loss: 0.7261 RMSE: 0.8521\n",
      "\n",
      "Epoch 74/150\n",
      "----------\n",
      "train Loss: 0.8518 RMSE: 0.9229\n",
      "val Loss: 0.7288 RMSE: 0.8537\n",
      "\n",
      "Epoch 75/150\n",
      "----------\n",
      "train Loss: 0.8524 RMSE: 0.9232\n",
      "val Loss: 0.7261 RMSE: 0.8521\n",
      "\n",
      "Epoch 76/150\n",
      "----------\n",
      "train Loss: 0.8508 RMSE: 0.9224\n",
      "val Loss: 0.7261 RMSE: 0.8521\n",
      "\n",
      "Epoch 77/150\n",
      "----------\n",
      "train Loss: 0.8496 RMSE: 0.9217\n",
      "val Loss: 0.7328 RMSE: 0.8560\n",
      "\n",
      "Epoch 78/150\n",
      "----------\n",
      "train Loss: 0.8495 RMSE: 0.9217\n",
      "val Loss: 0.7291 RMSE: 0.8539\n",
      "\n",
      "Epoch 79/150\n",
      "----------\n",
      "train Loss: 0.8480 RMSE: 0.9209\n",
      "val Loss: 0.7275 RMSE: 0.8529\n",
      "\n",
      "Epoch 80/150\n",
      "----------\n",
      "train Loss: 0.8472 RMSE: 0.9204\n",
      "val Loss: 0.7257 RMSE: 0.8519\n",
      "\n",
      "Epoch 81/150\n",
      "----------\n",
      "train Loss: 0.8473 RMSE: 0.9205\n",
      "val Loss: 0.7262 RMSE: 0.8521\n",
      "\n",
      "Epoch 82/150\n",
      "----------\n",
      "train Loss: 0.8465 RMSE: 0.9201\n",
      "val Loss: 0.7252 RMSE: 0.8516\n",
      "\n",
      "Epoch 83/150\n",
      "----------\n",
      "train Loss: 0.8441 RMSE: 0.9187\n",
      "val Loss: 0.7265 RMSE: 0.8524\n",
      "\n",
      "Epoch 84/150\n",
      "----------\n",
      "train Loss: 0.8444 RMSE: 0.9189\n",
      "val Loss: 0.7279 RMSE: 0.8532\n",
      "\n",
      "Epoch 85/150\n",
      "----------\n",
      "train Loss: 0.8436 RMSE: 0.9185\n",
      "val Loss: 0.7269 RMSE: 0.8526\n",
      "\n",
      "Epoch 86/150\n",
      "----------\n",
      "train Loss: 0.8430 RMSE: 0.9181\n",
      "val Loss: 0.7237 RMSE: 0.8507\n",
      "\n",
      "Epoch 87/150\n",
      "----------\n",
      "train Loss: 0.8414 RMSE: 0.9173\n",
      "val Loss: 0.7285 RMSE: 0.8535\n",
      "\n",
      "Epoch 88/150\n",
      "----------\n",
      "train Loss: 0.8407 RMSE: 0.9169\n",
      "val Loss: 0.7239 RMSE: 0.8509\n",
      "\n",
      "Epoch 89/150\n",
      "----------\n",
      "train Loss: 0.8409 RMSE: 0.9170\n",
      "val Loss: 0.7357 RMSE: 0.8577\n",
      "\n",
      "Epoch 90/150\n",
      "----------\n",
      "train Loss: 0.8408 RMSE: 0.9170\n",
      "val Loss: 0.7290 RMSE: 0.8538\n",
      "\n",
      "Epoch 91/150\n",
      "----------\n",
      "train Loss: 0.8384 RMSE: 0.9157\n",
      "val Loss: 0.7274 RMSE: 0.8529\n",
      "\n",
      "Epoch 92/150\n",
      "----------\n",
      "train Loss: 0.8377 RMSE: 0.9153\n",
      "val Loss: 0.7259 RMSE: 0.8520\n",
      "\n",
      "Epoch 93/150\n",
      "----------\n",
      "train Loss: 0.8363 RMSE: 0.9145\n",
      "val Loss: 0.7354 RMSE: 0.8575\n",
      "\n",
      "Epoch 94/150\n",
      "----------\n",
      "train Loss: 0.8376 RMSE: 0.9152\n",
      "val Loss: 0.7333 RMSE: 0.8563\n",
      "\n",
      "Epoch 95/150\n",
      "----------\n",
      "train Loss: 0.8361 RMSE: 0.9144\n",
      "val Loss: 0.7256 RMSE: 0.8518\n",
      "\n",
      "Epoch 96/150\n",
      "----------\n",
      "train Loss: 0.8347 RMSE: 0.9136\n",
      "val Loss: 0.7280 RMSE: 0.8532\n",
      "\n",
      "Epoch 97/150\n",
      "----------\n",
      "train Loss: 0.8348 RMSE: 0.9137\n",
      "val Loss: 0.7252 RMSE: 0.8516\n",
      "\n",
      "Epoch 98/150\n",
      "----------\n",
      "train Loss: 0.8337 RMSE: 0.9131\n",
      "val Loss: 0.7255 RMSE: 0.8517\n",
      "\n",
      "Epoch 99/150\n",
      "----------\n",
      "train Loss: 0.8321 RMSE: 0.9122\n",
      "val Loss: 0.7295 RMSE: 0.8541\n",
      "\n",
      "Epoch 100/150\n",
      "----------\n",
      "train Loss: 0.8312 RMSE: 0.9117\n",
      "val Loss: 0.7374 RMSE: 0.8587\n",
      "\n",
      "Epoch 101/150\n",
      "----------\n",
      "train Loss: 0.8313 RMSE: 0.9118\n",
      "val Loss: 0.7335 RMSE: 0.8564\n",
      "\n",
      "Epoch 102/150\n",
      "----------\n",
      "train Loss: 0.8305 RMSE: 0.9113\n",
      "val Loss: 0.7328 RMSE: 0.8560\n",
      "\n",
      "Epoch 103/150\n",
      "----------\n",
      "train Loss: 0.8299 RMSE: 0.9110\n",
      "val Loss: 0.7340 RMSE: 0.8567\n",
      "\n",
      "Epoch 104/150\n",
      "----------\n",
      "train Loss: 0.8304 RMSE: 0.9112\n",
      "val Loss: 0.7265 RMSE: 0.8524\n",
      "\n",
      "Epoch 105/150\n",
      "----------\n",
      "train Loss: 0.8296 RMSE: 0.9108\n",
      "val Loss: 0.7341 RMSE: 0.8568\n",
      "\n",
      "Epoch 106/150\n",
      "----------\n",
      "train Loss: 0.8294 RMSE: 0.9107\n",
      "val Loss: 0.7239 RMSE: 0.8508\n",
      "\n",
      "Epoch 107/150\n",
      "----------\n",
      "train Loss: 0.8285 RMSE: 0.9102\n",
      "val Loss: 0.7305 RMSE: 0.8547\n",
      "\n",
      "Epoch 108/150\n",
      "----------\n",
      "train Loss: 0.8278 RMSE: 0.9098\n",
      "val Loss: 0.7310 RMSE: 0.8550\n",
      "\n",
      "Epoch 109/150\n",
      "----------\n",
      "train Loss: 0.8264 RMSE: 0.9091\n",
      "val Loss: 0.7304 RMSE: 0.8546\n",
      "\n",
      "Epoch 110/150\n",
      "----------\n",
      "train Loss: 0.8255 RMSE: 0.9086\n",
      "val Loss: 0.7284 RMSE: 0.8535\n",
      "\n",
      "Epoch 111/150\n",
      "----------\n",
      "train Loss: 0.8248 RMSE: 0.9082\n",
      "val Loss: 0.7315 RMSE: 0.8553\n",
      "\n",
      "Epoch 112/150\n",
      "----------\n",
      "train Loss: 0.8241 RMSE: 0.9078\n",
      "val Loss: 0.7300 RMSE: 0.8544\n",
      "\n",
      "Epoch 113/150\n",
      "----------\n",
      "train Loss: 0.8245 RMSE: 0.9080\n",
      "val Loss: 0.7278 RMSE: 0.8531\n",
      "\n",
      "Epoch 114/150\n",
      "----------\n",
      "train Loss: 0.8237 RMSE: 0.9076\n",
      "val Loss: 0.7336 RMSE: 0.8565\n",
      "\n",
      "Epoch 115/150\n",
      "----------\n",
      "train Loss: 0.8238 RMSE: 0.9076\n",
      "val Loss: 0.7338 RMSE: 0.8566\n",
      "\n",
      "Epoch 116/150\n",
      "----------\n",
      "train Loss: 0.8229 RMSE: 0.9072\n",
      "val Loss: 0.7317 RMSE: 0.8554\n",
      "\n",
      "Epoch 117/150\n",
      "----------\n",
      "train Loss: 0.8244 RMSE: 0.9079\n",
      "val Loss: 0.7410 RMSE: 0.8608\n",
      "\n",
      "Epoch 118/150\n",
      "----------\n",
      "train Loss: 0.8211 RMSE: 0.9061\n",
      "val Loss: 0.7280 RMSE: 0.8532\n",
      "\n",
      "Epoch 119/150\n",
      "----------\n",
      "train Loss: 0.8234 RMSE: 0.9074\n",
      "val Loss: 0.7277 RMSE: 0.8530\n",
      "\n",
      "Epoch 120/150\n",
      "----------\n",
      "train Loss: 0.8229 RMSE: 0.9071\n",
      "val Loss: 0.7439 RMSE: 0.8625\n",
      "\n",
      "Epoch 121/150\n",
      "----------\n",
      "train Loss: 0.8207 RMSE: 0.9059\n",
      "val Loss: 0.7310 RMSE: 0.8550\n",
      "\n",
      "Epoch 122/150\n",
      "----------\n",
      "train Loss: 0.8240 RMSE: 0.9077\n",
      "val Loss: 0.7257 RMSE: 0.8519\n",
      "\n",
      "Epoch 123/150\n",
      "----------\n",
      "train Loss: 0.8230 RMSE: 0.9072\n",
      "val Loss: 0.7369 RMSE: 0.8584\n",
      "\n",
      "Epoch 124/150\n",
      "----------\n",
      "train Loss: 0.8216 RMSE: 0.9064\n",
      "val Loss: 0.7314 RMSE: 0.8552\n",
      "\n",
      "Epoch 125/150\n",
      "----------\n",
      "train Loss: 0.8206 RMSE: 0.9059\n",
      "val Loss: 0.7281 RMSE: 0.8533\n",
      "\n",
      "Epoch 126/150\n",
      "----------\n",
      "train Loss: 0.8193 RMSE: 0.9051\n",
      "val Loss: 0.7260 RMSE: 0.8520\n",
      "\n",
      "Epoch 127/150\n",
      "----------\n",
      "train Loss: 0.8192 RMSE: 0.9051\n",
      "val Loss: 0.7283 RMSE: 0.8534\n",
      "\n",
      "Epoch 128/150\n",
      "----------\n",
      "train Loss: 0.8187 RMSE: 0.9048\n",
      "val Loss: 0.7355 RMSE: 0.8576\n",
      "\n",
      "Epoch 129/150\n",
      "----------\n",
      "train Loss: 0.8193 RMSE: 0.9051\n",
      "val Loss: 0.7327 RMSE: 0.8560\n",
      "\n",
      "Epoch 130/150\n",
      "----------\n",
      "train Loss: 0.8180 RMSE: 0.9045\n",
      "val Loss: 0.7351 RMSE: 0.8574\n",
      "\n",
      "Epoch 131/150\n",
      "----------\n",
      "train Loss: 0.8163 RMSE: 0.9035\n",
      "val Loss: 0.7322 RMSE: 0.8557\n",
      "\n",
      "Epoch 132/150\n",
      "----------\n",
      "train Loss: 0.8174 RMSE: 0.9041\n",
      "val Loss: 0.7348 RMSE: 0.8572\n",
      "\n",
      "Epoch 133/150\n",
      "----------\n",
      "train Loss: 0.8180 RMSE: 0.9044\n",
      "val Loss: 0.7276 RMSE: 0.8530\n",
      "\n",
      "Epoch 134/150\n",
      "----------\n",
      "train Loss: 0.8156 RMSE: 0.9031\n",
      "val Loss: 0.7306 RMSE: 0.8548\n",
      "\n",
      "Epoch 135/150\n",
      "----------\n",
      "train Loss: 0.8143 RMSE: 0.9024\n",
      "val Loss: 0.7313 RMSE: 0.8552\n",
      "\n",
      "Epoch 136/150\n",
      "----------\n",
      "train Loss: 0.8142 RMSE: 0.9023\n",
      "val Loss: 0.7267 RMSE: 0.8525\n",
      "\n",
      "Epoch 137/150\n",
      "----------\n",
      "train Loss: 0.8147 RMSE: 0.9026\n",
      "val Loss: 0.7255 RMSE: 0.8518\n",
      "\n",
      "Epoch 138/150\n",
      "----------\n",
      "train Loss: 0.8131 RMSE: 0.9017\n",
      "val Loss: 0.7310 RMSE: 0.8550\n",
      "\n",
      "Epoch 139/150\n",
      "----------\n",
      "train Loss: 0.8140 RMSE: 0.9022\n",
      "val Loss: 0.7404 RMSE: 0.8604\n",
      "\n",
      "Epoch 140/150\n",
      "----------\n",
      "train Loss: 0.8126 RMSE: 0.9015\n",
      "val Loss: 0.7336 RMSE: 0.8565\n",
      "\n",
      "Epoch 141/150\n",
      "----------\n",
      "train Loss: 0.8137 RMSE: 0.9020\n",
      "val Loss: 0.7281 RMSE: 0.8533\n",
      "\n",
      "Epoch 142/150\n",
      "----------\n",
      "train Loss: 0.8127 RMSE: 0.9015\n",
      "val Loss: 0.7320 RMSE: 0.8556\n",
      "\n",
      "Epoch 143/150\n",
      "----------\n",
      "train Loss: 0.8149 RMSE: 0.9027\n",
      "val Loss: 0.7415 RMSE: 0.8611\n",
      "\n",
      "Epoch 144/150\n",
      "----------\n",
      "train Loss: 0.8128 RMSE: 0.9015\n",
      "val Loss: 0.7280 RMSE: 0.8533\n",
      "\n",
      "Epoch 145/150\n",
      "----------\n",
      "train Loss: 0.8125 RMSE: 0.9014\n",
      "val Loss: 0.7299 RMSE: 0.8543\n",
      "\n",
      "Epoch 146/150\n",
      "----------\n",
      "train Loss: 0.8108 RMSE: 0.9005\n",
      "val Loss: 0.7339 RMSE: 0.8567\n",
      "\n",
      "Epoch 147/150\n",
      "----------\n",
      "train Loss: 0.8106 RMSE: 0.9004\n",
      "val Loss: 0.7329 RMSE: 0.8561\n",
      "\n",
      "Epoch 148/150\n",
      "----------\n",
      "train Loss: 0.8109 RMSE: 0.9005\n",
      "val Loss: 0.7373 RMSE: 0.8587\n",
      "\n",
      "Epoch 149/150\n",
      "----------\n",
      "train Loss: 0.8104 RMSE: 0.9002\n",
      "val Loss: 0.7335 RMSE: 0.8565\n",
      "\n",
      "Epoch 150/150\n",
      "----------\n",
      "train Loss: 0.8096 RMSE: 0.8998\n",
      "val Loss: 0.7392 RMSE: 0.8598\n",
      "\n",
      "Training complete in 0m 7s\n",
      "Best val RMSE: 0.850725\n",
      "Start testing data\n",
      "\n",
      "MSE : 7496.346896737321 , MAE : 49.428399543131675\n"
     ]
    }
   ],
   "source": [
    "# Case 1. model = lstm\n",
    "config = config1\n",
    "data_forecast = mf.Forecasting(config, train_data, test_data, test_date)\n",
    "init_model = data_forecast.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_forecast.train_model(init_model)  # 모델 학습\n",
    "    data_forecast.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse, mae = data_forecast.pred_data(init_model, best_model_path=config[\"best_model_path\"])  # 예측\n",
    "print(f'MSE : {mse} , MAE : {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "toxic-bradford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model\n",
      "\n",
      "Epoch 1/150\n",
      "----------\n",
      "train Loss: 1.0045 RMSE: 1.0023\n",
      "val Loss: 0.9157 RMSE: 0.9569\n",
      "\n",
      "Epoch 2/150\n",
      "----------\n",
      "train Loss: 1.0001 RMSE: 1.0001\n",
      "val Loss: 0.9103 RMSE: 0.9541\n",
      "\n",
      "Epoch 3/150\n",
      "----------\n",
      "train Loss: 0.9957 RMSE: 0.9978\n",
      "val Loss: 0.9053 RMSE: 0.9515\n",
      "\n",
      "Epoch 4/150\n",
      "----------\n",
      "train Loss: 0.9917 RMSE: 0.9959\n",
      "val Loss: 0.9001 RMSE: 0.9487\n",
      "\n",
      "Epoch 5/150\n",
      "----------\n",
      "train Loss: 0.9878 RMSE: 0.9939\n",
      "val Loss: 0.8947 RMSE: 0.9459\n",
      "\n",
      "Epoch 6/150\n",
      "----------\n",
      "train Loss: 0.9835 RMSE: 0.9917\n",
      "val Loss: 0.8891 RMSE: 0.9429\n",
      "\n",
      "Epoch 7/150\n",
      "----------\n",
      "train Loss: 0.9793 RMSE: 0.9896\n",
      "val Loss: 0.8832 RMSE: 0.9398\n",
      "\n",
      "Epoch 8/150\n",
      "----------\n",
      "train Loss: 0.9749 RMSE: 0.9873\n",
      "val Loss: 0.8773 RMSE: 0.9367\n",
      "\n",
      "Epoch 9/150\n",
      "----------\n",
      "train Loss: 0.9706 RMSE: 0.9852\n",
      "val Loss: 0.8709 RMSE: 0.9332\n",
      "\n",
      "Epoch 10/150\n",
      "----------\n",
      "train Loss: 0.9656 RMSE: 0.9827\n",
      "val Loss: 0.8641 RMSE: 0.9296\n",
      "\n",
      "Epoch 11/150\n",
      "----------\n",
      "train Loss: 0.9605 RMSE: 0.9801\n",
      "val Loss: 0.8563 RMSE: 0.9253\n",
      "\n",
      "Epoch 12/150\n",
      "----------\n",
      "train Loss: 0.9545 RMSE: 0.9770\n",
      "val Loss: 0.8472 RMSE: 0.9204\n",
      "\n",
      "Epoch 13/150\n",
      "----------\n",
      "train Loss: 0.9482 RMSE: 0.9737\n",
      "val Loss: 0.8363 RMSE: 0.9145\n",
      "\n",
      "Epoch 14/150\n",
      "----------\n",
      "train Loss: 0.9401 RMSE: 0.9696\n",
      "val Loss: 0.8255 RMSE: 0.9086\n",
      "\n",
      "Epoch 15/150\n",
      "----------\n",
      "train Loss: 0.9328 RMSE: 0.9658\n",
      "val Loss: 0.8137 RMSE: 0.9021\n",
      "\n",
      "Epoch 16/150\n",
      "----------\n",
      "train Loss: 0.9241 RMSE: 0.9613\n",
      "val Loss: 0.8009 RMSE: 0.8949\n",
      "\n",
      "Epoch 17/150\n",
      "----------\n",
      "train Loss: 0.9161 RMSE: 0.9571\n",
      "val Loss: 0.7876 RMSE: 0.8875\n",
      "\n",
      "Epoch 18/150\n",
      "----------\n",
      "train Loss: 0.9076 RMSE: 0.9527\n",
      "val Loss: 0.7758 RMSE: 0.8808\n",
      "\n",
      "Epoch 19/150\n",
      "----------\n",
      "train Loss: 0.9006 RMSE: 0.9490\n",
      "val Loss: 0.7657 RMSE: 0.8750\n",
      "\n",
      "Epoch 20/150\n",
      "----------\n",
      "train Loss: 0.8952 RMSE: 0.9462\n",
      "val Loss: 0.7578 RMSE: 0.8705\n",
      "\n",
      "Epoch 21/150\n",
      "----------\n",
      "train Loss: 0.8906 RMSE: 0.9437\n",
      "val Loss: 0.7518 RMSE: 0.8670\n",
      "\n",
      "Epoch 22/150\n",
      "----------\n",
      "train Loss: 0.8883 RMSE: 0.9425\n",
      "val Loss: 0.7470 RMSE: 0.8643\n",
      "\n",
      "Epoch 23/150\n",
      "----------\n",
      "train Loss: 0.8860 RMSE: 0.9413\n",
      "val Loss: 0.7449 RMSE: 0.8631\n",
      "\n",
      "Epoch 24/150\n",
      "----------\n",
      "train Loss: 0.8839 RMSE: 0.9402\n",
      "val Loss: 0.7433 RMSE: 0.8621\n",
      "\n",
      "Epoch 25/150\n",
      "----------\n",
      "train Loss: 0.8830 RMSE: 0.9397\n",
      "val Loss: 0.7425 RMSE: 0.8617\n",
      "\n",
      "Epoch 26/150\n",
      "----------\n",
      "train Loss: 0.8819 RMSE: 0.9391\n",
      "val Loss: 0.7414 RMSE: 0.8611\n",
      "\n",
      "Epoch 27/150\n",
      "----------\n",
      "train Loss: 0.8808 RMSE: 0.9385\n",
      "val Loss: 0.7387 RMSE: 0.8595\n",
      "\n",
      "Epoch 28/150\n",
      "----------\n",
      "train Loss: 0.8800 RMSE: 0.9381\n",
      "val Loss: 0.7359 RMSE: 0.8579\n",
      "\n",
      "Epoch 29/150\n",
      "----------\n",
      "train Loss: 0.8793 RMSE: 0.9377\n",
      "val Loss: 0.7351 RMSE: 0.8574\n",
      "\n",
      "Epoch 30/150\n",
      "----------\n",
      "train Loss: 0.8788 RMSE: 0.9374\n",
      "val Loss: 0.7335 RMSE: 0.8565\n",
      "\n",
      "Epoch 31/150\n",
      "----------\n",
      "train Loss: 0.8782 RMSE: 0.9371\n",
      "val Loss: 0.7328 RMSE: 0.8560\n",
      "\n",
      "Epoch 32/150\n",
      "----------\n",
      "train Loss: 0.8779 RMSE: 0.9370\n",
      "val Loss: 0.7314 RMSE: 0.8552\n",
      "\n",
      "Epoch 33/150\n",
      "----------\n",
      "train Loss: 0.8772 RMSE: 0.9366\n",
      "val Loss: 0.7311 RMSE: 0.8550\n",
      "\n",
      "Epoch 34/150\n",
      "----------\n",
      "train Loss: 0.8764 RMSE: 0.9361\n",
      "val Loss: 0.7308 RMSE: 0.8549\n",
      "\n",
      "Epoch 35/150\n",
      "----------\n",
      "train Loss: 0.8759 RMSE: 0.9359\n",
      "val Loss: 0.7313 RMSE: 0.8551\n",
      "\n",
      "Epoch 36/150\n",
      "----------\n",
      "train Loss: 0.8754 RMSE: 0.9356\n",
      "val Loss: 0.7317 RMSE: 0.8554\n",
      "\n",
      "Epoch 37/150\n",
      "----------\n",
      "train Loss: 0.8750 RMSE: 0.9354\n",
      "val Loss: 0.7310 RMSE: 0.8550\n",
      "\n",
      "Epoch 38/150\n",
      "----------\n",
      "train Loss: 0.8745 RMSE: 0.9351\n",
      "val Loss: 0.7290 RMSE: 0.8538\n",
      "\n",
      "Epoch 39/150\n",
      "----------\n",
      "train Loss: 0.8740 RMSE: 0.9349\n",
      "val Loss: 0.7278 RMSE: 0.8531\n",
      "\n",
      "Epoch 40/150\n",
      "----------\n",
      "train Loss: 0.8739 RMSE: 0.9348\n",
      "val Loss: 0.7272 RMSE: 0.8527\n",
      "\n",
      "Epoch 41/150\n",
      "----------\n",
      "train Loss: 0.8730 RMSE: 0.9344\n",
      "val Loss: 0.7270 RMSE: 0.8527\n",
      "\n",
      "Epoch 42/150\n",
      "----------\n",
      "train Loss: 0.8725 RMSE: 0.9341\n",
      "val Loss: 0.7274 RMSE: 0.8529\n",
      "\n",
      "Epoch 43/150\n",
      "----------\n",
      "train Loss: 0.8721 RMSE: 0.9339\n",
      "val Loss: 0.7267 RMSE: 0.8525\n",
      "\n",
      "Epoch 44/150\n",
      "----------\n",
      "train Loss: 0.8719 RMSE: 0.9338\n",
      "val Loss: 0.7282 RMSE: 0.8533\n",
      "\n",
      "Epoch 45/150\n",
      "----------\n",
      "train Loss: 0.8716 RMSE: 0.9336\n",
      "val Loss: 0.7281 RMSE: 0.8533\n",
      "\n",
      "Epoch 46/150\n",
      "----------\n",
      "train Loss: 0.8709 RMSE: 0.9332\n",
      "val Loss: 0.7257 RMSE: 0.8519\n",
      "\n",
      "Epoch 47/150\n",
      "----------\n",
      "train Loss: 0.8704 RMSE: 0.9329\n",
      "val Loss: 0.7236 RMSE: 0.8507\n",
      "\n",
      "Epoch 48/150\n",
      "----------\n",
      "train Loss: 0.8701 RMSE: 0.9328\n",
      "val Loss: 0.7223 RMSE: 0.8499\n",
      "\n",
      "Epoch 49/150\n",
      "----------\n",
      "train Loss: 0.8697 RMSE: 0.9326\n",
      "val Loss: 0.7217 RMSE: 0.8495\n",
      "\n",
      "Epoch 50/150\n",
      "----------\n",
      "train Loss: 0.8692 RMSE: 0.9323\n",
      "val Loss: 0.7218 RMSE: 0.8496\n",
      "\n",
      "Epoch 51/150\n",
      "----------\n",
      "train Loss: 0.8688 RMSE: 0.9321\n",
      "val Loss: 0.7219 RMSE: 0.8497\n",
      "\n",
      "Epoch 52/150\n",
      "----------\n",
      "train Loss: 0.8685 RMSE: 0.9319\n",
      "val Loss: 0.7229 RMSE: 0.8502\n",
      "\n",
      "Epoch 53/150\n",
      "----------\n",
      "train Loss: 0.8682 RMSE: 0.9318\n",
      "val Loss: 0.7227 RMSE: 0.8501\n",
      "\n",
      "Epoch 54/150\n",
      "----------\n",
      "train Loss: 0.8677 RMSE: 0.9315\n",
      "val Loss: 0.7221 RMSE: 0.8497\n",
      "\n",
      "Epoch 55/150\n",
      "----------\n",
      "train Loss: 0.8674 RMSE: 0.9314\n",
      "val Loss: 0.7217 RMSE: 0.8495\n",
      "\n",
      "Epoch 56/150\n",
      "----------\n",
      "train Loss: 0.8671 RMSE: 0.9312\n",
      "val Loss: 0.7199 RMSE: 0.8485\n",
      "\n",
      "Epoch 57/150\n",
      "----------\n",
      "train Loss: 0.8669 RMSE: 0.9311\n",
      "val Loss: 0.7199 RMSE: 0.8485\n",
      "\n",
      "Epoch 58/150\n",
      "----------\n",
      "train Loss: 0.8664 RMSE: 0.9308\n",
      "val Loss: 0.7204 RMSE: 0.8487\n",
      "\n",
      "Epoch 59/150\n",
      "----------\n",
      "train Loss: 0.8657 RMSE: 0.9304\n",
      "val Loss: 0.7204 RMSE: 0.8488\n",
      "\n",
      "Epoch 60/150\n",
      "----------\n",
      "train Loss: 0.8653 RMSE: 0.9302\n",
      "val Loss: 0.7206 RMSE: 0.8489\n",
      "\n",
      "Epoch 61/150\n",
      "----------\n",
      "train Loss: 0.8648 RMSE: 0.9300\n",
      "val Loss: 0.7206 RMSE: 0.8489\n",
      "\n",
      "Epoch 62/150\n",
      "----------\n",
      "train Loss: 0.8647 RMSE: 0.9299\n",
      "val Loss: 0.7199 RMSE: 0.8485\n",
      "\n",
      "Epoch 63/150\n",
      "----------\n",
      "train Loss: 0.8641 RMSE: 0.9295\n",
      "val Loss: 0.7211 RMSE: 0.8491\n",
      "\n",
      "Epoch 64/150\n",
      "----------\n",
      "train Loss: 0.8636 RMSE: 0.9293\n",
      "val Loss: 0.7216 RMSE: 0.8494\n",
      "\n",
      "Epoch 65/150\n",
      "----------\n",
      "train Loss: 0.8632 RMSE: 0.9291\n",
      "val Loss: 0.7219 RMSE: 0.8497\n",
      "\n",
      "Epoch 66/150\n",
      "----------\n",
      "train Loss: 0.8629 RMSE: 0.9289\n",
      "val Loss: 0.7218 RMSE: 0.8496\n",
      "\n",
      "Epoch 67/150\n",
      "----------\n",
      "train Loss: 0.8625 RMSE: 0.9287\n",
      "val Loss: 0.7211 RMSE: 0.8492\n",
      "\n",
      "Epoch 68/150\n",
      "----------\n",
      "train Loss: 0.8622 RMSE: 0.9285\n",
      "val Loss: 0.7212 RMSE: 0.8492\n",
      "\n",
      "Epoch 69/150\n",
      "----------\n",
      "train Loss: 0.8617 RMSE: 0.9283\n",
      "val Loss: 0.7206 RMSE: 0.8489\n",
      "\n",
      "Epoch 70/150\n",
      "----------\n",
      "train Loss: 0.8613 RMSE: 0.9280\n",
      "val Loss: 0.7201 RMSE: 0.8486\n",
      "\n",
      "Epoch 71/150\n",
      "----------\n",
      "train Loss: 0.8610 RMSE: 0.9279\n",
      "val Loss: 0.7220 RMSE: 0.8497\n",
      "\n",
      "Epoch 72/150\n",
      "----------\n",
      "train Loss: 0.8606 RMSE: 0.9277\n",
      "val Loss: 0.7226 RMSE: 0.8501\n",
      "\n",
      "Epoch 73/150\n",
      "----------\n",
      "train Loss: 0.8601 RMSE: 0.9274\n",
      "val Loss: 0.7218 RMSE: 0.8496\n",
      "\n",
      "Epoch 74/150\n",
      "----------\n",
      "train Loss: 0.8598 RMSE: 0.9273\n",
      "val Loss: 0.7197 RMSE: 0.8484\n",
      "\n",
      "Epoch 75/150\n",
      "----------\n",
      "train Loss: 0.8591 RMSE: 0.9269\n",
      "val Loss: 0.7197 RMSE: 0.8484\n",
      "\n",
      "Epoch 76/150\n",
      "----------\n",
      "train Loss: 0.8586 RMSE: 0.9266\n",
      "val Loss: 0.7207 RMSE: 0.8489\n",
      "\n",
      "Epoch 77/150\n",
      "----------\n",
      "train Loss: 0.8582 RMSE: 0.9264\n",
      "val Loss: 0.7217 RMSE: 0.8495\n",
      "\n",
      "Epoch 78/150\n",
      "----------\n",
      "train Loss: 0.8578 RMSE: 0.9261\n",
      "val Loss: 0.7211 RMSE: 0.8492\n",
      "\n",
      "Epoch 79/150\n",
      "----------\n",
      "train Loss: 0.8573 RMSE: 0.9259\n",
      "val Loss: 0.7235 RMSE: 0.8506\n",
      "\n",
      "Epoch 80/150\n",
      "----------\n",
      "train Loss: 0.8567 RMSE: 0.9256\n",
      "val Loss: 0.7216 RMSE: 0.8495\n",
      "\n",
      "Epoch 81/150\n",
      "----------\n",
      "train Loss: 0.8563 RMSE: 0.9254\n",
      "val Loss: 0.7196 RMSE: 0.8483\n",
      "\n",
      "Epoch 82/150\n",
      "----------\n",
      "train Loss: 0.8558 RMSE: 0.9251\n",
      "val Loss: 0.7195 RMSE: 0.8482\n",
      "\n",
      "Epoch 83/150\n",
      "----------\n",
      "train Loss: 0.8551 RMSE: 0.9247\n",
      "val Loss: 0.7200 RMSE: 0.8485\n",
      "\n",
      "Epoch 84/150\n",
      "----------\n",
      "train Loss: 0.8550 RMSE: 0.9247\n",
      "val Loss: 0.7219 RMSE: 0.8496\n",
      "\n",
      "Epoch 85/150\n",
      "----------\n",
      "train Loss: 0.8545 RMSE: 0.9244\n",
      "val Loss: 0.7206 RMSE: 0.8489\n",
      "\n",
      "Epoch 86/150\n",
      "----------\n",
      "train Loss: 0.8535 RMSE: 0.9239\n",
      "val Loss: 0.7203 RMSE: 0.8487\n",
      "\n",
      "Epoch 87/150\n",
      "----------\n",
      "train Loss: 0.8530 RMSE: 0.9236\n",
      "val Loss: 0.7203 RMSE: 0.8487\n",
      "\n",
      "Epoch 88/150\n",
      "----------\n",
      "train Loss: 0.8524 RMSE: 0.9233\n",
      "val Loss: 0.7199 RMSE: 0.8485\n",
      "\n",
      "Epoch 89/150\n",
      "----------\n",
      "train Loss: 0.8521 RMSE: 0.9231\n",
      "val Loss: 0.7193 RMSE: 0.8481\n",
      "\n",
      "Epoch 90/150\n",
      "----------\n",
      "train Loss: 0.8518 RMSE: 0.9229\n",
      "val Loss: 0.7195 RMSE: 0.8482\n",
      "\n",
      "Epoch 91/150\n",
      "----------\n",
      "train Loss: 0.8510 RMSE: 0.9225\n",
      "val Loss: 0.7194 RMSE: 0.8482\n",
      "\n",
      "Epoch 92/150\n",
      "----------\n",
      "train Loss: 0.8506 RMSE: 0.9223\n",
      "val Loss: 0.7191 RMSE: 0.8480\n",
      "\n",
      "Epoch 93/150\n",
      "----------\n",
      "train Loss: 0.8500 RMSE: 0.9220\n",
      "val Loss: 0.7180 RMSE: 0.8474\n",
      "\n",
      "Epoch 94/150\n",
      "----------\n",
      "train Loss: 0.8495 RMSE: 0.9217\n",
      "val Loss: 0.7203 RMSE: 0.8487\n",
      "\n",
      "Epoch 95/150\n",
      "----------\n",
      "train Loss: 0.8485 RMSE: 0.9211\n",
      "val Loss: 0.7198 RMSE: 0.8484\n",
      "\n",
      "Epoch 96/150\n",
      "----------\n",
      "train Loss: 0.8479 RMSE: 0.9208\n",
      "val Loss: 0.7212 RMSE: 0.8492\n",
      "\n",
      "Epoch 97/150\n",
      "----------\n",
      "train Loss: 0.8472 RMSE: 0.9204\n",
      "val Loss: 0.7219 RMSE: 0.8496\n",
      "\n",
      "Epoch 98/150\n",
      "----------\n",
      "train Loss: 0.8468 RMSE: 0.9202\n",
      "val Loss: 0.7218 RMSE: 0.8496\n",
      "\n",
      "Epoch 99/150\n",
      "----------\n",
      "train Loss: 0.8460 RMSE: 0.9198\n",
      "val Loss: 0.7197 RMSE: 0.8484\n",
      "\n",
      "Epoch 100/150\n",
      "----------\n",
      "train Loss: 0.8455 RMSE: 0.9195\n",
      "val Loss: 0.7217 RMSE: 0.8495\n",
      "\n",
      "Epoch 101/150\n",
      "----------\n",
      "train Loss: 0.8446 RMSE: 0.9190\n",
      "val Loss: 0.7247 RMSE: 0.8513\n",
      "\n",
      "Epoch 102/150\n",
      "----------\n",
      "train Loss: 0.8450 RMSE: 0.9193\n",
      "val Loss: 0.7217 RMSE: 0.8495\n",
      "\n",
      "Epoch 103/150\n",
      "----------\n",
      "train Loss: 0.8434 RMSE: 0.9184\n",
      "val Loss: 0.7175 RMSE: 0.8471\n",
      "\n",
      "Epoch 104/150\n",
      "----------\n",
      "train Loss: 0.8430 RMSE: 0.9182\n",
      "val Loss: 0.7185 RMSE: 0.8476\n",
      "\n",
      "Epoch 105/150\n",
      "----------\n",
      "train Loss: 0.8418 RMSE: 0.9175\n",
      "val Loss: 0.7202 RMSE: 0.8486\n",
      "\n",
      "Epoch 106/150\n",
      "----------\n",
      "train Loss: 0.8413 RMSE: 0.9172\n",
      "val Loss: 0.7232 RMSE: 0.8504\n",
      "\n",
      "Epoch 107/150\n",
      "----------\n",
      "train Loss: 0.8409 RMSE: 0.9170\n",
      "val Loss: 0.7220 RMSE: 0.8497\n",
      "\n",
      "Epoch 108/150\n",
      "----------\n",
      "train Loss: 0.8399 RMSE: 0.9165\n",
      "val Loss: 0.7208 RMSE: 0.8490\n",
      "\n",
      "Epoch 109/150\n",
      "----------\n",
      "train Loss: 0.8390 RMSE: 0.9160\n",
      "val Loss: 0.7234 RMSE: 0.8505\n",
      "\n",
      "Epoch 110/150\n",
      "----------\n",
      "train Loss: 0.8382 RMSE: 0.9155\n",
      "val Loss: 0.7209 RMSE: 0.8491\n",
      "\n",
      "Epoch 111/150\n",
      "----------\n",
      "train Loss: 0.8377 RMSE: 0.9152\n",
      "val Loss: 0.7194 RMSE: 0.8482\n",
      "\n",
      "Epoch 112/150\n",
      "----------\n",
      "train Loss: 0.8368 RMSE: 0.9148\n",
      "val Loss: 0.7200 RMSE: 0.8486\n",
      "\n",
      "Epoch 113/150\n",
      "----------\n",
      "train Loss: 0.8362 RMSE: 0.9144\n",
      "val Loss: 0.7190 RMSE: 0.8480\n",
      "\n",
      "Epoch 114/150\n",
      "----------\n",
      "train Loss: 0.8353 RMSE: 0.9139\n",
      "val Loss: 0.7203 RMSE: 0.8487\n",
      "\n",
      "Epoch 115/150\n",
      "----------\n",
      "train Loss: 0.8345 RMSE: 0.9135\n",
      "val Loss: 0.7198 RMSE: 0.8484\n",
      "\n",
      "Epoch 116/150\n",
      "----------\n",
      "train Loss: 0.8337 RMSE: 0.9131\n",
      "val Loss: 0.7192 RMSE: 0.8480\n",
      "\n",
      "Epoch 117/150\n",
      "----------\n",
      "train Loss: 0.8334 RMSE: 0.9129\n",
      "val Loss: 0.7221 RMSE: 0.8498\n",
      "\n",
      "Epoch 118/150\n",
      "----------\n",
      "train Loss: 0.8326 RMSE: 0.9125\n",
      "val Loss: 0.7214 RMSE: 0.8493\n",
      "\n",
      "Epoch 119/150\n",
      "----------\n",
      "train Loss: 0.8322 RMSE: 0.9123\n",
      "val Loss: 0.7227 RMSE: 0.8501\n",
      "\n",
      "Epoch 120/150\n",
      "----------\n",
      "train Loss: 0.8319 RMSE: 0.9121\n",
      "val Loss: 0.7192 RMSE: 0.8480\n",
      "\n",
      "Epoch 121/150\n",
      "----------\n",
      "train Loss: 0.8312 RMSE: 0.9117\n",
      "val Loss: 0.7223 RMSE: 0.8499\n",
      "\n",
      "Epoch 122/150\n",
      "----------\n",
      "train Loss: 0.8304 RMSE: 0.9112\n",
      "val Loss: 0.7194 RMSE: 0.8482\n",
      "\n",
      "Epoch 123/150\n",
      "----------\n",
      "train Loss: 0.8299 RMSE: 0.9110\n",
      "val Loss: 0.7187 RMSE: 0.8478\n",
      "\n",
      "Epoch 124/150\n",
      "----------\n",
      "train Loss: 0.8290 RMSE: 0.9105\n",
      "val Loss: 0.7224 RMSE: 0.8499\n",
      "\n",
      "Epoch 125/150\n",
      "----------\n",
      "train Loss: 0.8286 RMSE: 0.9103\n",
      "val Loss: 0.7215 RMSE: 0.8494\n",
      "\n",
      "Epoch 126/150\n",
      "----------\n",
      "train Loss: 0.8282 RMSE: 0.9100\n",
      "val Loss: 0.7235 RMSE: 0.8506\n",
      "\n",
      "Epoch 127/150\n",
      "----------\n",
      "train Loss: 0.8278 RMSE: 0.9098\n",
      "val Loss: 0.7213 RMSE: 0.8493\n",
      "\n",
      "Epoch 128/150\n",
      "----------\n",
      "train Loss: 0.8272 RMSE: 0.9095\n",
      "val Loss: 0.7232 RMSE: 0.8504\n",
      "\n",
      "Epoch 129/150\n",
      "----------\n",
      "train Loss: 0.8266 RMSE: 0.9092\n",
      "val Loss: 0.7230 RMSE: 0.8503\n",
      "\n",
      "Epoch 130/150\n",
      "----------\n",
      "train Loss: 0.8260 RMSE: 0.9089\n",
      "val Loss: 0.7218 RMSE: 0.8496\n",
      "\n",
      "Epoch 131/150\n",
      "----------\n",
      "train Loss: 0.8255 RMSE: 0.9085\n",
      "val Loss: 0.7215 RMSE: 0.8494\n",
      "\n",
      "Epoch 132/150\n",
      "----------\n",
      "train Loss: 0.8253 RMSE: 0.9084\n",
      "val Loss: 0.7222 RMSE: 0.8498\n",
      "\n",
      "Epoch 133/150\n",
      "----------\n",
      "train Loss: 0.8248 RMSE: 0.9082\n",
      "val Loss: 0.7213 RMSE: 0.8493\n",
      "\n",
      "Epoch 134/150\n",
      "----------\n",
      "train Loss: 0.8251 RMSE: 0.9083\n",
      "val Loss: 0.7175 RMSE: 0.8471\n",
      "\n",
      "Epoch 135/150\n",
      "----------\n",
      "train Loss: 0.8243 RMSE: 0.9079\n",
      "val Loss: 0.7217 RMSE: 0.8495\n",
      "\n",
      "Epoch 136/150\n",
      "----------\n",
      "train Loss: 0.8239 RMSE: 0.9077\n",
      "val Loss: 0.7228 RMSE: 0.8502\n",
      "\n",
      "Epoch 137/150\n",
      "----------\n",
      "train Loss: 0.8236 RMSE: 0.9075\n",
      "val Loss: 0.7215 RMSE: 0.8494\n",
      "\n",
      "Epoch 138/150\n",
      "----------\n",
      "train Loss: 0.8235 RMSE: 0.9075\n",
      "val Loss: 0.7231 RMSE: 0.8503\n",
      "\n",
      "Epoch 139/150\n",
      "----------\n",
      "train Loss: 0.8229 RMSE: 0.9072\n",
      "val Loss: 0.7235 RMSE: 0.8506\n",
      "\n",
      "Epoch 140/150\n",
      "----------\n",
      "train Loss: 0.8230 RMSE: 0.9072\n",
      "val Loss: 0.7270 RMSE: 0.8527\n",
      "\n",
      "Epoch 141/150\n",
      "----------\n",
      "train Loss: 0.8222 RMSE: 0.9067\n",
      "val Loss: 0.7213 RMSE: 0.8493\n",
      "\n",
      "Epoch 142/150\n",
      "----------\n",
      "train Loss: 0.8217 RMSE: 0.9065\n",
      "val Loss: 0.7200 RMSE: 0.8485\n",
      "\n",
      "Epoch 143/150\n",
      "----------\n",
      "train Loss: 0.8209 RMSE: 0.9060\n",
      "val Loss: 0.7256 RMSE: 0.8518\n",
      "\n",
      "Epoch 144/150\n",
      "----------\n",
      "train Loss: 0.8208 RMSE: 0.9060\n",
      "val Loss: 0.7220 RMSE: 0.8497\n",
      "\n",
      "Epoch 145/150\n",
      "----------\n",
      "train Loss: 0.8203 RMSE: 0.9057\n",
      "val Loss: 0.7179 RMSE: 0.8473\n",
      "\n",
      "Epoch 146/150\n",
      "----------\n",
      "train Loss: 0.8212 RMSE: 0.9062\n",
      "val Loss: 0.7175 RMSE: 0.8471\n",
      "\n",
      "Epoch 147/150\n",
      "----------\n",
      "train Loss: 0.8196 RMSE: 0.9053\n",
      "val Loss: 0.7241 RMSE: 0.8510\n",
      "\n",
      "Epoch 148/150\n",
      "----------\n",
      "train Loss: 0.8205 RMSE: 0.9058\n",
      "val Loss: 0.7246 RMSE: 0.8512\n",
      "\n",
      "Epoch 149/150\n",
      "----------\n",
      "train Loss: 0.8190 RMSE: 0.9050\n",
      "val Loss: 0.7180 RMSE: 0.8474\n",
      "\n",
      "Epoch 150/150\n",
      "----------\n",
      "train Loss: 0.8194 RMSE: 0.9052\n",
      "val Loss: 0.7169 RMSE: 0.8467\n",
      "\n",
      "Training complete in 0m 7s\n",
      "Best val RMSE: 0.846701\n",
      "Start testing data\n",
      "\n",
      "MSE : 7445.102915662169 , MAE : 47.53524722869074\n"
     ]
    }
   ],
   "source": [
    "# Case 2. model = gru\n",
    "config = config2\n",
    "data_forecast = mf.Forecasting(config, train_data, test_data, test_date)\n",
    "init_model = data_forecast.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_forecast.train_model(init_model)  # 모델 학습\n",
    "    data_forecast.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse, mae = data_forecast.pred_data(init_model, best_model_path=config[\"best_model_path\"]) # 예측\n",
    "print(f'MSE : {mse} , MAE : {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 3. model = informer\n",
    "config = config3\n",
    "data_forecast = mf.Forecasting(config, train_data, test_data, test_date)\n",
    "init_model = data_forecast.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_forecast.train_model(init_model)  # 모델 학습\n",
    "    data_forecast.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse, mae = data_forecast.pred_data(init_model, best_model_path=config[\"best_model_path\"])  # 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 4. model = scinet\n",
    "config = config4\n",
    "data_forecast = mf.Forecasting(config, train_data, test_data, test_date)\n",
    "init_model = data_forecast.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_forecast.train_model(init_model)  # 모델 학습\n",
    "    data_forecast.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse, mae = data_forecast.pred_data(init_model, best_model_path=config[\"best_model_path\"])  # 예측"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 2.7.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
