{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "similar-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import main_forecasting as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "mature-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "exterior-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1. model = lstm\n",
    "config1 = {\n",
    "    \"model\": 'lstm',\n",
    "    \"training\": True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "    \"best_model_path\": './ckpt/lstm.pt',  # 학습 완료 모델 저장 경로\n",
    "    \"parameter\": {\n",
    "        \"input_size\" : 1,  # 데이터 변수 개수, int\n",
    "        \"window_size\" : 48,  # input sequence의 길이, int\n",
    "        \"forecast_step\" : 24,  # 예측할 미래 시점의 길이, int\n",
    "        \"num_layers\" : 2,  # recurrent layers의 수, int(default: 2, 범위: 1 이상)\n",
    "        \"hidden_size\" : 64,  # hidden state의 차원, int(default: 64, 범위: 1 이상)\n",
    "        \"dropout\" : 0.1,  # dropout 확률, float(default: 0.1, 범위: 0 이상 1 이하)\n",
    "        \"bidirectional\" : True,  # 모델의 양방향성 여부, bool(default: True)\n",
    "        \"num_epochs\" : 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "        \"batch_size\" : 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "        \"lr\" : 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "        \"device\" : 'cuda'  # 학습 환경, (default: 'cuda', ['cuda', 'cpu'] 중 선택)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Case 2. model = gru\n",
    "config2 = {\n",
    "    \"model\": 'gru',\n",
    "    \"training\": True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "    \"best_model_path\": './ckpt/gru.pt',  # 학습 완료 모델 저장 경로\n",
    "    \"parameter\": {\n",
    "        \"input_size\" : 1,  # 데이터 변수 개수, int\n",
    "        \"window_size\" : 48,  # input sequence의 길이, int\n",
    "        \"forecast_step\" : 24,  # 예측할 미래 시점의 길이, int\n",
    "        \"num_layers\" : 2,  # recurrent layers의 수, int(default: 2, 범위: 1 이상)\n",
    "        \"hidden_size\" : 64,  # hidden state의 차원, int(default: 64, 범위: 1 이상)\n",
    "        \"dropout\" : 0.1,  # dropout 확률, float(default: 0.1, 범위: 0 이상 1 이하)\n",
    "        \"bidirectional\" : True,  # 모델의 양방향성 여부, bool(default: True)\n",
    "        \"num_epochs\" : 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "        \"batch_size\" : 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "        \"lr\" : 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "        \"device\" : 'cuda'  # 학습 환경, (default: 'cuda', ['cuda', 'cpu'] 중 선택)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Case 3. model = informer\n",
    "config3 = {\n",
    "    \"model\": 'informer',\n",
    "    \"training\": True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "    \"best_model_path\": './ckpt/informer.pt',  # 학습 완료 모델 저장 경로\n",
    "    \"parameter\": {\n",
    "        \"input_size\" : 1,  # 데이터 변수 개수, int\n",
    "        \"window_size\" : 48,  # input sequence의 길이, int\n",
    "        \"forecast_step\" : 24,  # 예측할 미래 시점의 길이, int\n",
    "        \"label_len\" : 12,  # Decoder의 start token 길이, int(default: 12)\n",
    "        \"d_model\" : 512,  # 모델의 hidden dimension, int(default: 512)\n",
    "        \"e_layers\" : 2,  # encoder layer 수, int(default: 2)\n",
    "        \"d_layers\" : 1,  # decoder layer 수, int(default: 1)\n",
    "        \"d_ff\" : 2048,  # fully connected layer의 hidden dimension, int(default: 2048)\n",
    "        \"num_epochs\" : 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "        \"batch_size\" : 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "        \"lr\" : 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "        \"device\" : 'cuda',  # 학습 환경, (default: 'cuda', ['cuda', 'cpu'] 중 선택)\n",
    "        \"factor\" : 5, # 모델의 ProbSparse Attention factor, int(default: 5)\n",
    "        \"dropout\" : 0.05, # dropout ratio, int(default: 0.05)\n",
    "        \"attn\" : 'prob', # 모델의 attention 계산 방식, (default: 'prob', ['prob', 'full'] 중 선택)\n",
    "        \"n_heads\" : 8, # multi-head attention head 수, int(default: 8)\n",
    "        \"embed\" : 'timeF', # time features encoding 방식, (default: 'timeF', ['timeF', 'fixed', 'learned'] 중 선택)\n",
    "        \"lradj\" : 'type1' # learning rate 조정 방식, (default: 'type1', ['type1', 'type2'] 중 선택)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Case 4. model = scinet\n",
    "config4 = {\n",
    "    \"model\": 'scinet',\n",
    "    \"training\": True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "    \"best_model_path\": './ckpt/scinet.pt',  # 학습 완료 모델 저장 경로\n",
    "    \"parameter\": {\n",
    "        \"input_size\" : 1,  # 데이터 변수 개수, int\n",
    "        \"window_size\" : 48,  # input sequence의 길이, int\n",
    "        \"forecast_step\" : 24,  # 예측할 미래 시점의 길이, int\n",
    "        \"levels\" : 2, # Tree의 depth, int(default: 2, 범위: input sequence의 로그 값 이하, 2~4 설정 권장)\n",
    "        \"stacks\" : 1, # SCINet 구조를 쌓는 횟수, int(default: 1, 범위: 3 이하)\n",
    "        \"num_epochs\" : 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "        \"batch_size\" : 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "        \"lr\" : 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "        \"device\" : 'cuda'  # 학습 환경, (default: 'cuda', ['cuda', 'cpu'] 중 선택)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "speaking-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset_dir = {\n",
    "    \"train\": './data/train_data.csv',\n",
    "    \"test\": './data/test_data.csv'\n",
    "}\n",
    "\n",
    "# train/test 데이터 불러오기 (csv 형태)\n",
    "# shape=(# time steps, )\n",
    "train_data = pd.read_csv(dataset_dir[\"train\"])\n",
    "train_data = train_data[\"Appliances\"].values\n",
    "\n",
    "test_data = pd.read_csv(dataset_dir[\"test\"])\n",
    "test_date = test_data[\"date\"].values\n",
    "test_data = test_data[\"Appliances\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1. model = lstm\n",
    "config = config1\n",
    "data_forecast = mf.Forecasting(config, train_data, test_data, test_date)\n",
    "init_model = data_forecast.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_forecast.train_model(init_model)  # 모델 학습\n",
    "    data_forecast.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse, mae = data_forecast.pred_data(init_model, best_model_path=config[\"best_model_path\"])  # 예측\n",
    "print(f'MSE : {mse} , MAE : {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2. model = gru\n",
    "config = config2\n",
    "data_forecast = mf.Forecasting(config, train_data, test_data, test_date)\n",
    "init_model = data_forecast.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_forecast.train_model(init_model)  # 모델 학습\n",
    "    data_forecast.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse, mae = data_forecast.pred_data(init_model, best_model_path=config[\"best_model_path\"]) # 예측\n",
    "print(f'MSE : {mse} , MAE : {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "minus-election",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 1 cost time: 0.26839232444763184\n",
      "Epoch: 1, Steps: 8 | Train Loss: 1.0407582 Vali Loss: 0.7340948\n",
      "Validation loss decreased (inf --> 0.734095)\n",
      "Updating learning rate to 0.0001\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 2 cost time: 0.2233562469482422\n",
      "Epoch: 2, Steps: 8 | Train Loss: 0.7788695 Vali Loss: 0.6571889\n",
      "Validation loss decreased (0.734095 --> 0.657189)\n",
      "Updating learning rate to 5e-05\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 3 cost time: 0.22437644004821777\n",
      "Epoch: 3, Steps: 8 | Train Loss: 0.7219350 Vali Loss: 0.6304239\n",
      "Validation loss decreased (0.657189 --> 0.630424)\n",
      "Updating learning rate to 2.5e-05\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 4 cost time: 0.2244408130645752\n",
      "Epoch: 4, Steps: 8 | Train Loss: 0.7389765 Vali Loss: 0.6359096\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 5 cost time: 0.22443938255310059\n",
      "Epoch: 5, Steps: 8 | Train Loss: 0.6307251 Vali Loss: 0.6126007\n",
      "Validation loss decreased (0.630424 --> 0.612601)\n",
      "Updating learning rate to 6.25e-06\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 6 cost time: 0.22520947456359863\n",
      "Epoch: 6, Steps: 8 | Train Loss: 0.6222994 Vali Loss: 0.6079186\n",
      "Validation loss decreased (0.612601 --> 0.607919)\n",
      "Updating learning rate to 3.125e-06\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 7 cost time: 0.22416329383850098\n",
      "Epoch: 7, Steps: 8 | Train Loss: 0.6507598 Vali Loss: 0.6123984\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 8 cost time: 0.22451090812683105\n",
      "Epoch: 8, Steps: 8 | Train Loss: 0.6545048 Vali Loss: 0.6155185\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      "-----------------------------------------------------------------\n",
      "Epoch: 9 cost time: 0.22432708740234375\n",
      "Epoch: 9, Steps: 8 | Train Loss: 0.6184238 Vali Loss: 0.6112506\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "Start testing data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Case 3. model = informer\n",
    "config = config3\n",
    "data_forecast = mf.Forecasting(config, train_data, test_data, test_date)\n",
    "init_model = data_forecast.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_forecast.train_model(init_model)  # 모델 학습\n",
    "    data_forecast.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse, mae = data_forecast.pred_data(init_model, best_model_path=config[\"best_model_path\"])  # 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 4. model = scinet\n",
    "config = config4\n",
    "data_forecast = mf.Forecasting(config, train_data, test_data, test_date)\n",
    "init_model = data_forecast.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_forecast.train_model(init_model)  # 모델 학습\n",
    "    data_forecast.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse, mae = data_forecast.pred_data(init_model, best_model_path=config[\"best_model_path\"])  # 예측"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 2.7.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
