{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import main_forecasting as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1. model = lstm\n",
    "config1 = {\n",
    "    \"model\": 'lstm',\n",
    "    \"training\": True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "    \"best_model_path\": './ckpt/lstm.pt',  # 학습 완료 모델 저장 경로\n",
    "    \"parameter\": {\n",
    "        \"input_size\" : 1,  # 데이터 변수 개수, int\n",
    "        \"window_size\" : 48,  # input sequence의 길이, int\n",
    "        \"forecast_step\" : 24,  # 예측할 미래 시점의 길이, int\n",
    "        \"num_layers\" : 2,  # recurrent layers의 수, int(default: 2, 범위: 1 이상)\n",
    "        \"hidden_size\" : 64,  # hidden state의 차원, int(default: 64, 범위: 1 이상)\n",
    "        \"dropout\" : 0.1,  # dropout 확률, float(default: 0.1, 범위: 0 이상 1 이하)\n",
    "        \"bidirectional\" : True,  # 모델의 양방향성 여부, bool(default: True)\n",
    "        \"num_epochs\" : 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "        \"batch_size\" : 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "        \"lr\" : 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "        \"device\" : 'cuda'  # 학습 환경, (default: 'cuda', ['cuda', 'cpu'] 중 선택)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Case 2. model = gru\n",
    "config2 = {\n",
    "    \"model\": 'gru',\n",
    "    \"training\": True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "    \"best_model_path\": './ckpt/gru.pt',  # 학습 완료 모델 저장 경로\n",
    "    \"parameter\": {\n",
    "        \"input_size\" : 1,  # 데이터 변수 개수, int\n",
    "        \"window_size\" : 48,  # input sequence의 길이, int\n",
    "        \"forecast_step\" : 24,  # 예측할 미래 시점의 길이, int\n",
    "        \"num_layers\" : 2,  # recurrent layers의 수, int(default: 2, 범위: 1 이상)\n",
    "        \"hidden_size\" : 64,  # hidden state의 차원, int(default: 64, 범위: 1 이상)\n",
    "        \"dropout\" : 0.1,  # dropout 확률, float(default: 0.1, 범위: 0 이상 1 이하)\n",
    "        \"bidirectional\" : True,  # 모델의 양방향성 여부, bool(default: True)\n",
    "        \"num_epochs\" : 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "        \"batch_size\" : 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "        \"lr\" : 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "        \"device\" : 'cuda'  # 학습 환경, (default: 'cuda', ['cuda', 'cpu'] 중 선택)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Case 3. model = informer\n",
    "config3 = {\n",
    "    \"model\": 'informer',\n",
    "    \"training\": True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "    \"best_model_path\": './ckpt/informer.pt',  # 학습 완료 모델 저장 경로\n",
    "    \"parameter\": {\n",
    "        \"input_size\" : 1,  # 데이터 변수 개수, int\n",
    "        \"window_size\" : 48,  # input sequence의 길이, int\n",
    "        \"forecast_step\" : 24,  # 예측할 미래 시점의 길이, int\n",
    "        \"label_len\" : 48,  # Decoder의 start token 길이, int(default: 48)\n",
    "        \"d_model\" : 512,  # 모델의 hidden dimension, int(default: 512)\n",
    "        \"e_layers\" : 2,  # encoder layer 수, int(default: 2)\n",
    "        \"d_layers\" : 1,  # decoder layer 수, int(default: 1)\n",
    "        \"d_ff\" : 2048,  # fully connected layer의 hidden dimension, int(default: 2048)\n",
    "        \"num_epochs\" : 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "        \"batch_size\" : 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "        \"lr\" : 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "        \"device\" : 'cuda'  # 학습 환경, (default: 'cuda', ['cuda', 'cpu'] 중 선택)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Case 4. model = scinet\n",
    "config4 = {\n",
    "    \"model\": 'scinet',\n",
    "    \"training\": True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "    \"best_model_path\": './ckpt/scinet.pt',  # 학습 완료 모델 저장 경로\n",
    "    \"parameter\": {\n",
    "        \"input_size\" : 1,  # 데이터 변수 개수, int\n",
    "        \"window_size\" : 48,  # input sequence의 길이, int\n",
    "        \"forecast_step\" : 24,  # 예측할 미래 시점의 길이, int\n",
    "        \"levels\" : 2, # Tree의 depth, int(default: 2, 범위: input sequence의 로그 값 이하, 2~4 설정 권장)\n",
    "        \"stacks\" : 1, # SCINet 구조를 쌓는 횟수, int(default: 1, 범위: 3 이하)\n",
    "        \"num_epochs\" : 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "        \"batch_size\" : 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "        \"lr\" : 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "        \"device\" : 'cuda'  # 학습 환경, (default: 'cuda', ['cuda', 'cpu'] 중 선택)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset_dir = {\n",
    "    \"train\": './data/train_data.csv',\n",
    "    \"test\": './data/test_data.csv'\n",
    "}\n",
    "\n",
    "# train/test 데이터 불러오기 (csv 형태)\n",
    "# shape=(# time steps, )\n",
    "train_data = pd.read_csv(dataset_dir[\"train\"])\n",
    "train_data = train_data[\"Appliances\"].values\n",
    "\n",
    "test_data = pd.read_csv(dataset_dir[\"test\"])\n",
    "test_date = test_data[\"date\"].values\n",
    "test_data = test_data[\"Appliances\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1. model = lstm\n",
    "config = config1\n",
    "data_forecast = mf.Forecasting(config, train_data, test_data, test_date)\n",
    "init_model = data_forecast.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_forecast.train_model(init_model)  # 모델 학습\n",
    "    data_forecast.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse, mae = data_forecast.pred_data(init_model, best_model_path=config[\"best_model_path\"])  # 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2. model = gru\n",
    "config = config2\n",
    "data_forecast = mf.Forecasting(config, train_data, test_data, test_date)\n",
    "init_model = data_forecast.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_forecast.train_model(init_model)  # 모델 학습\n",
    "    data_forecast.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse, mae = data_forecast.pred_data(init_model, best_model_path=config[\"best_model_path\"])  # 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 3. model = informer\n",
    "config = config3\n",
    "data_forecast = mf.Forecasting(config, train_data, test_data, test_date)\n",
    "init_model = data_forecast.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_forecast.train_model(init_model)  # 모델 학습\n",
    "    data_forecast.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse, mae = data_forecast.pred_data(init_model, best_model_path=config[\"best_model_path\"])  # 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 4. model = scinet\n",
    "config = config4\n",
    "data_forecast = mf.Forecasting(config, train_data, test_data, test_date)\n",
    "init_model = data_forecast.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_forecast.train_model(init_model)  # 모델 학습\n",
    "    data_forecast.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse, mae = data_forecast.pred_data(init_model, best_model_path=config[\"best_model_path\"])  # 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af2d87a9baae9e19220b6d245f822c980479669c4aad7c3aadfe7a700f0cdbad"
  },
  "kernelspec": {
   "display_name": "iitp_time_serise",
   "language": "python",
   "name": "iitp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
